{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the **label** based on the **text_** column, you can follow a standard **text classification pipeline** using machine learning. Here’s the process:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Data Preprocessing**\n",
    "- Remove unnecessary punctuation and special characters.\n",
    "- Convert text to lowercase.\n",
    "- Tokenization (split text into words).\n",
    "- Remove stopwords (common words like \"the,\" \"and,\" etc.).\n",
    "- Lemmatization (convert words to their base forms, e.g., \"loved\" → \"love\").\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Feature Engineering**\n",
    "- Convert text into numerical format using **TF-IDF** (Term Frequency-Inverse Document Frequency) or **Word Embeddings** (like Word2Vec, BERT, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Model Selection**\n",
    "- Use a **Machine Learning model** like:\n",
    "  - **Logistic Regression** (Good baseline)\n",
    "  - **Naive Bayes** (Performs well on text classification)\n",
    "  - **Random Forest**\n",
    "  - **SVM (Support Vector Machine)**\n",
    "  - **Deep Learning (LSTM, BERT, etc.)** for more complex cases.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Training the Model**\n",
    "- Split data into training (80%) and testing (20%).\n",
    "- Train the model on the training data.\n",
    "- Evaluate using metrics like **accuracy, precision, recall, and F1-score**.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Predicting Labels**\n",
    "- Once the model is trained, pass new reviews through the same preprocessing steps.\n",
    "- Use the trained model to predict the label.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Deployment**\n",
    "- Deploy using Flask, FastAPI, or Django if needed for real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mradu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mradu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mradu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import DataPipeline as dp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Data Preprocessing**\n",
    "- Remove unnecessary punctuation and special characters.\n",
    "- Convert text to lowercase.\n",
    "- Tokenization (split text into words).\n",
    "- Remove stopwords (common words like \"the,\" \"and,\" etc.).\n",
    "- Lemmatization (convert words to their base forms, e.g., \"loved\" → \"love\").\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Feature Engineering**\n",
    "- Convert text into numerical format using **TF-IDF** (Term Frequency-Inverse Document Frequency) or **Word Embeddings** (like Word2Vec, BERT, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40432 entries, 0 to 40431\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   category  40432 non-null  object \n",
      " 1   rating    40432 non-null  float64\n",
      " 2   label     40432 non-null  object \n",
      " 3   text_     40432 non-null  object \n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category    0\n",
       "rating      0\n",
       "label       0\n",
       "text_       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "rating",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "733cb503-bfa3-4571-bf5b-ed5b78c1c839",
       "rows": [
        [
         "count",
         "40432.0"
        ],
        [
         "mean",
         "4.256578947368421"
        ],
        [
         "std",
         "1.1443539194684895"
        ],
        [
         "min",
         "1.0"
        ],
        [
         "25%",
         "4.0"
        ],
        [
         "50%",
         "5.0"
        ],
        [
         "75%",
         "5.0"
        ],
        [
         "max",
         "5.0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>40432.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.256579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.144354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             rating\n",
       "count  40432.000000\n",
       "mean       4.256579\n",
       "std        1.144354\n",
       "min        1.000000\n",
       "25%        4.000000\n",
       "50%        5.000000\n",
       "75%        5.000000\n",
       "max        5.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mradu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mradu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mradu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, return_tokens=True):\n",
    "    \"\"\"\n",
    "    Cleans and tokenizes text data.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        list: Tokenized and lemmatized words.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or pd.isna(text):  \n",
    "        return []  # Return an empty list if text is NaN or not a string\n",
    "\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "    text = text.lower().strip()  # Convert to lowercase and remove extra spaces\n",
    "    \n",
    "    if not return_tokens:\n",
    "        return text  # Return cleaned text if not tokenizing\n",
    "    \n",
    "    tokens = word_tokenize(text)  # Tokenize\n",
    "\n",
    "    # Remove stopwords using set operation (faster lookup)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text data\n",
    "tokenized_texts = df['text_'].apply(preprocess_text).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love', 'great', 'upgrade', 'original', 'ive', 'mine', 'couple', 'year']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11607667, 12778950)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.build_vocab(tokenized_texts)\n",
    "word2vec_model.train(tokenized_texts, total_examples=len(tokenized_texts), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'love':\n",
      "[ 1.5967331  -0.56937474  1.0252733  -1.2883037   2.8542306   0.44401702\n",
      "  1.8470085  -1.2330059  -0.64124006  0.29404876 -1.0456376  -0.16074888\n",
      "  2.569506    0.28775528  0.46492323  2.3119192   0.8722445   0.8755863\n",
      " -2.4262805  -0.27493778  0.18199678  0.18548332  1.5814512  -0.81522316\n",
      "  0.5897589  -0.3603973   1.3619702   0.60154814 -2.0165274   0.37490317\n",
      "  0.8194316  -1.2665157  -1.6206524   0.41881898 -1.3718954   0.7925512\n",
      " -0.12078405  2.2625754   2.7491891  -0.17060392 -0.83384675 -0.13375951\n",
      " -1.4296724   2.574428    0.7458072   1.1308368  -0.56630296 -0.73024046\n",
      " -0.5677177  -0.0308758   0.90575296 -0.54529804 -0.30168638 -1.2305642\n",
      " -2.1559942   0.30665514  0.98258424 -0.83023864 -0.3905523  -0.28582767\n",
      " -1.1266986  -0.00940782  1.8499604   2.2290757  -0.02287936  3.4112456\n",
      "  1.2255063   1.3963727   1.1385323  -0.8806984  -0.91929567 -0.57344663\n",
      "  0.86584085 -0.2299536  -1.88281     3.0921242  -0.10281567 -0.01431896\n",
      " -2.812101   -1.2451503  -0.50343573 -1.7021121   0.25760695  1.9022768\n",
      " -0.21490854 -0.9550416   0.36262435 -0.7005527   1.3929892   0.21837983\n",
      " -3.1618776   0.2978582  -1.7945045  -1.0123875   0.18779902 -0.20312248\n",
      "  2.0248106   1.4493254   1.3225524  -0.46890998]\n"
     ]
    }
   ],
   "source": [
    "# Get word vectors\n",
    "word_vectors = word2vec_model.wv\n",
    "\n",
    "# Example: Get vector for the word \"love\"\n",
    "print(\"Vector for 'love':\")\n",
    "print(word_vectors[\"love\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words similar to 'love':\n",
      "[('loved', 0.6791303157806396), ('cute', 0.5656462907791138), ('great', 0.5520269274711609), ('boredgreat', 0.5236473083496094), ('adorable', 0.5178564786911011), ('awesome', 0.5136257410049438), ('like', 0.5134742856025696), ('everyonethis', 0.49875909090042114), ('liked', 0.4940889775753021), ('happy', 0.48140522837638855)]\n"
     ]
    }
   ],
   "source": [
    "# Example: Find similar words to \"love\"\n",
    "print(\"\\nWords similar to 'love':\")\n",
    "print(word_vectors.most_similar(\"love\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sentence_to_vector(tokens, word2vec_model, vector_size=100):\n",
    "    \"\"\"\n",
    "    Converts a tokenized sentence into a numerical vector using Word2Vec.\n",
    "\n",
    "    Args:\n",
    "        tokens (list): List of preprocessed words (tokens).\n",
    "        word2vec_model (Word2Vec): A trained Word2Vec model.\n",
    "        vector_size (int): Size of the word vectors (default: 100).\n",
    "\n",
    "    Returns:\n",
    "        np.array: The sentence vector.\n",
    "    \"\"\"\n",
    "    word_vectors = [word2vec_model.wv[word] for word in tokens if word in word2vec_model.wv]\n",
    "    # print(word_vectors)  # Debugging: Check the word vectors\n",
    "    \n",
    "    if not word_vectors:  # If no valid words in the sentence\n",
    "        return np.zeros(vector_size)  \n",
    "    \n",
    "    return np.mean(word_vectors, axis=0)  # Compute the mean vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['text_'].apply(preprocess_text)  # First preprocess text\n",
    "df['text_vector'] = df['tokens'].apply(lambda x: sentence_to_vector(x, word2vec_model))  # Then vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "category",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "rating",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text_",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tokens",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "text_vector",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "70872402-bc62-4c8d-8926-b2273cc28574",
       "rows": [
        [
         "0",
         "Home_and_Kitchen_5",
         "5.0",
         "CG",
         "Love this!  Well made, sturdy, and very comfortable.  I love it!Very pretty",
         "['love', 'well', 'made', 'sturdy', 'comfortable', 'love', 'itvery', 'pretty']",
         "[ 1.5406663  -0.99293655  0.98273134 -1.073451    1.5940106   0.08900366\n  1.9863657  -0.9216271   0.97505677 -0.52925056 -0.03071048 -0.57340384\n  0.6212532  -0.38576794 -0.19824754  1.3653005   0.5973764   1.1641116\n -2.2505643  -0.10270691 -0.7416421  -0.16475351 -0.17206123 -0.68026435\n  0.1312781   0.43391392  1.0432307   0.58014077 -0.80275023 -0.4893574\n -0.16453278 -0.4323914  -0.5587864  -1.0535343  -1.0088116   0.09867649\n -0.07285484  0.02944589  1.8640083   0.19103982 -0.2108796  -0.59048676\n -0.9078429   1.2671754   0.98466384 -0.7034508   0.5213939  -0.624285\n -0.6580008  -0.09484892  0.4753011  -0.59085685  0.69107145 -1.0615911\n -0.7727326   0.7106334  -0.28880212 -0.03919759 -0.40217122 -0.034102\n -0.01516341 -0.56098264  1.168522    1.2816669  -0.41677344  1.5472785\n  0.5252798   1.1741484   0.3670716  -0.51278335 -1.1686392  -0.6497339\n  0.94539905 -0.82829213 -0.33339     1.8562038   0.14499035  1.3487568\n -1.2378563  -1.2601649   0.48875362 -0.14940995  0.4341461   1.9636779\n  0.5939158  -0.5543149   0.5712019   0.09741798  0.6198935   0.6094583\n -0.17699176  0.12575415 -0.34819928 -0.47505417  0.32033327  0.11553097\n  1.4558601   0.910872    0.01833394 -0.5281719 ]"
        ],
        [
         "1",
         "Home_and_Kitchen_5",
         "5.0",
         "CG",
         "love it, a great upgrade from the original.  I've had mine for a couple of years",
         "['love', 'great', 'upgrade', 'original', 'ive', 'mine', 'couple', 'year']",
         "[ 0.56903756 -0.5683802   0.12395129 -0.30268204  0.7866783   0.04115747\n  1.2438099  -0.9468616   0.6740805  -0.44073454  0.36875138 -0.01942585\n  0.08378315  0.23350322  0.97179145  0.6139443   0.6680447  -0.9943022\n -1.2923391  -0.30028263  0.3002839  -0.64587986  0.68813914 -0.6572478\n -0.04357744  0.6125671   0.5221036  -0.06663918  0.0217279  -0.87857103\n  0.2621802  -0.17322119  0.15223297 -0.5438683  -0.31145188  0.760701\n -0.5554929   0.8806567   0.07041091  0.2015307  -0.2407094  -0.40187812\n  0.4564556   0.99809575  0.4200011   0.48968324  0.82964766 -0.33596075\n -0.88414276 -0.58041286  1.5708773   0.27517796  0.05083448 -0.4694368\n  0.01291993 -0.13865863 -0.07822689 -0.52827877 -0.64752054  0.81956315\n -0.29107076 -0.04598415  0.42708844  0.2683383  -0.7081431   1.1391854\n -0.5735483   0.9240724  -0.19269702 -0.1613326  -1.2558906  -1.7064673\n  0.27447835  0.27979118 -0.09799711  0.8991318   0.40292946  0.9813689\n -0.8839126  -1.2511624  -0.33520907  0.9869812   0.36169884  0.34317303\n -0.5193575  -0.7548355   0.7245533   0.27876028  0.5564014   0.27576098\n -0.6530553  -0.34771287 -0.5136066   0.02267557  0.05966777  0.50639164\n  1.1839532   0.8509707  -0.32596612 -1.3707496 ]"
        ],
        [
         "2",
         "Home_and_Kitchen_5",
         "5.0",
         "CG",
         "This pillow saved my back. I love the look and feel of this pillow.",
         "['pillow', 'saved', 'back', 'love', 'look', 'feel', 'pillow']",
         "[ 0.57553756  0.44632658  0.48697212 -0.73144895  0.67402    -1.0579935\n  0.9216522  -0.3702778   0.5194556  -0.2112223   0.19152549 -1.2614995\n  1.033503   -0.20473994  0.56319433  0.50023484 -0.7324118   0.18321612\n -1.4267156   0.6422316  -0.9362198  -0.25821608 -0.10914391 -0.5057054\n  0.410798   -0.43179396  1.136243    0.57847214  0.5948027  -0.5546383\n  0.3315733  -0.1449769  -0.5495905  -0.11018454 -0.7759958   0.6051722\n  0.82252365 -1.1098614   0.3268129  -0.49184194 -0.7062632  -0.24703434\n -1.0954597  -0.20926173 -0.44012162 -0.4214273  -0.71585304 -0.28982094\n -0.2931149  -0.97670084  1.4140189   0.14667137  0.44010285 -0.5797039\n -0.24138823  0.6917106   0.03007722 -0.15840673  0.769267    0.41937307\n -0.32186764 -0.08596527  0.01310723  1.1149278  -0.3312181   2.0792472\n  0.3135936   0.1414211  -0.7243236  -0.2630886  -0.24974538 -1.4882421\n  0.8815365   0.26402965 -0.4713826   0.8510787  -0.3564214  -0.90306324\n -0.9346372  -0.85233086  0.13120763  0.2571346   0.19127104  0.7500676\n  0.03721363  0.39276776  1.0046475   0.00783958  2.0099518  -0.23985335\n  0.09722525  1.0688964  -0.03766826  0.7436959   1.0899284  -0.13617963\n  0.4915487   0.08241593  0.02282121 -0.55074805]"
        ],
        [
         "3",
         "Home_and_Kitchen_5",
         "1.0",
         "CG",
         "Missing information on how to use it, but it is a great product for the price!  I",
         "['missing', 'information', 'use', 'great', 'product', 'price']",
         "[ 0.764389   -0.9699766  -0.633388   -0.5582797   0.05696286 -0.7357462\n  2.1720407  -0.03956652 -0.03770837 -0.32197356  0.40771484 -0.5452697\n -0.13238497 -0.14257018 -0.34505162  1.2717662   1.1618243  -0.1153275\n -1.3483204  -0.36153224  0.5616948  -1.3652071   0.3517832  -0.8625029\n -0.8327368  -0.2653633   0.31637153 -0.01490405  0.6179956  -0.3028931\n  0.381923    0.0076292   0.12733488 -0.16457473  0.36955693  0.55821997\n  0.07057374  0.8082077   0.8835952  -0.47698233 -0.22982685 -0.90381604\n -0.41568854 -1.1336237   0.12058381 -0.16509686  0.7203786  -0.01791747\n -0.3387253  -0.15435968  0.9475195  -0.01764753 -0.9822473   0.00827873\n  0.24905328  1.9841204  -0.5366053  -0.7840207  -0.38270643  0.0110875\n  0.27842477  0.15691622  0.73386663 -0.2928699  -0.10235635  0.08261611\n  0.7418494   1.4025602   0.26179656 -0.44189754 -1.0220946  -1.3111244\n  0.3274347   0.16254634 -0.82224244 -0.32274738 -0.6686828   0.6708872\n -1.2728611  -1.2330645   0.14901279  0.1712485   0.5660427   1.1693431\n -0.3894144  -0.7711413  -0.45433235  0.5370093   0.05697864 -0.0109428\n  1.249771   -1.3197225   0.00992564 -0.35161158 -0.10587201  0.1698113\n  0.37118372  0.31569877 -0.6317304  -1.0973364 ]"
        ],
        [
         "4",
         "Home_and_Kitchen_5",
         "5.0",
         "CG",
         "Very nice set. Good quality. We have had the set for two months now and have not been",
         "['nice', 'set', 'good', 'quality', 'set', 'two', 'month']",
         "[ 1.05609667e+00 -7.23997176e-01  6.42272085e-02 -9.06203449e-01\n  9.54255879e-01 -2.95751542e-01  1.88863623e+00  2.22278503e-03\n  1.04894245e+00 -9.92247462e-01  6.33787513e-01 -4.17911530e-01\n  4.59548861e-01 -8.14551115e-02 -3.70190293e-01  5.94794214e-01\n  3.59676868e-01 -1.85678586e-01 -1.12690818e+00 -2.21104234e-01\n -4.86511379e-01 -1.36835980e+00  6.47853374e-01 -5.24309576e-01\n -2.69359678e-01  4.60840851e-01  7.79913783e-01  1.16923702e+00\n -5.91380537e-01 -1.12069011e+00  5.10969102e-01 -5.15998900e-01\n -4.55089986e-01 -6.43098176e-01  1.53041080e-01  9.90275323e-01\n  2.45205848e-03  8.11171651e-01  9.90072906e-01  1.43116117e+00\n -5.91281891e-01 -4.10149768e-02 -9.88404095e-01  1.23892747e-01\n  3.34153146e-01  1.56203791e-01  1.21080983e+00 -3.40570241e-01\n -6.41777873e-01 -4.13689464e-01  1.65686429e+00  4.57942754e-01\n -1.40279651e-01 -8.62505078e-01  1.42294362e-01  3.36166918e-01\n  7.45487958e-02  9.26919639e-01  1.39477223e-01  5.00986040e-01\n -5.08194923e-01  1.99953333e-01  4.70722139e-01  7.50075400e-01\n -9.66955185e-01  3.93896520e-01  3.62373024e-01  1.03735089e+00\n -3.00810546e-01 -7.43850529e-01 -8.89674604e-01 -1.85382950e+00\n  8.03418279e-01 -2.42519617e-01 -4.38152216e-02  3.61141026e-01\n -6.17327690e-01  1.78536606e+00 -1.06037569e+00 -2.18623018e+00\n  1.18842773e-01  5.48435867e-01  1.05901492e+00  2.29518151e+00\n  4.58474934e-01  2.50416219e-01  3.84880990e-01  6.98283970e-01\n -2.51442164e-01  1.07772574e-01 -2.70793647e-01 -4.31022644e-01\n  8.92994031e-02 -8.41160193e-02 -2.11839184e-01  2.95943707e-01\n  1.45045578e+00  1.20521283e+00 -7.67954528e-01 -8.53826225e-01]"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>text_</th>\n",
       "      <th>tokens</th>\n",
       "      <th>text_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Love this!  Well made, sturdy, and very comfor...</td>\n",
       "      <td>[love, well, made, sturdy, comfortable, love, ...</td>\n",
       "      <td>[1.5406663, -0.99293655, 0.98273134, -1.073451...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>love it, a great upgrade from the original.  I...</td>\n",
       "      <td>[love, great, upgrade, original, ive, mine, co...</td>\n",
       "      <td>[0.56903756, -0.5683802, 0.123951286, -0.30268...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>This pillow saved my back. I love the look and...</td>\n",
       "      <td>[pillow, saved, back, love, look, feel, pillow]</td>\n",
       "      <td>[0.57553756, 0.44632658, 0.48697212, -0.731448...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Missing information on how to use it, but it i...</td>\n",
       "      <td>[missing, information, use, great, product, pr...</td>\n",
       "      <td>[0.764389, -0.9699766, -0.633388, -0.5582797, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Very nice set. Good quality. We have had the s...</td>\n",
       "      <td>[nice, set, good, quality, set, two, month]</td>\n",
       "      <td>[1.0560967, -0.7239972, 0.06422721, -0.9062034...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             category  rating label  \\\n",
       "0  Home_and_Kitchen_5     5.0    CG   \n",
       "1  Home_and_Kitchen_5     5.0    CG   \n",
       "2  Home_and_Kitchen_5     5.0    CG   \n",
       "3  Home_and_Kitchen_5     1.0    CG   \n",
       "4  Home_and_Kitchen_5     5.0    CG   \n",
       "\n",
       "                                               text_  \\\n",
       "0  Love this!  Well made, sturdy, and very comfor...   \n",
       "1  love it, a great upgrade from the original.  I...   \n",
       "2  This pillow saved my back. I love the look and...   \n",
       "3  Missing information on how to use it, but it i...   \n",
       "4  Very nice set. Good quality. We have had the s...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [love, well, made, sturdy, comfortable, love, ...   \n",
       "1  [love, great, upgrade, original, ive, mine, co...   \n",
       "2    [pillow, saved, back, love, look, feel, pillow]   \n",
       "3  [missing, information, use, great, product, pr...   \n",
       "4        [nice, set, good, quality, set, two, month]   \n",
       "\n",
       "                                         text_vector  \n",
       "0  [1.5406663, -0.99293655, 0.98273134, -1.073451...  \n",
       "1  [0.56903756, -0.5683802, 0.123951286, -0.30268...  \n",
       "2  [0.57553756, 0.44632658, 0.48697212, -0.731448...  \n",
       "3  [0.764389, -0.9699766, -0.633388, -0.5582797, ...  \n",
       "4  [1.0560967, -0.7239972, 0.06422721, -0.9062034...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix Shape: (40432, 1000)\n",
      "Feature Names: ['able' 'absolutely' 'accurate' 'acting' 'action' 'actor' 'actually'\n",
      " 'adapter' 'add' 'added' 'addition' 'adjust' 'adjustable' 'admit' 'adult'\n",
      " 'adventure' 'advertised' 'age' 'ago' 'air' 'almost' 'alone' 'along'\n",
      " 'already' 'also' 'although' 'always' 'amazing' 'amazon' 'american'\n",
      " 'amount' 'animal' 'annoying' 'another' 'anyone' 'anything' 'apart' 'arc'\n",
      " 'area' 'arent' 'arm' 'around' 'arrived' 'art' 'assemble' 'attention'\n",
      " 'author' 'available' 'away' 'awesome' 'baby' 'back' 'bad' 'bag' 'ball'\n",
      " 'band' 'bar' 'base' 'based' 'basic' 'bathroom' 'battery' 'beautiful'\n",
      " 'become' 'bed' 'beginning' 'behind' 'believable' 'believe' 'belt' 'best'\n",
      " 'better' 'big' 'bigger' 'bike' 'birthday' 'bit' 'black' 'blade' 'blue'\n",
      " 'board' 'body' 'book' 'booki' 'boot' 'boring' 'bosch' 'bottle' 'bottom'\n",
      " 'bought' 'bowl' 'box' 'boy' 'bra' 'brand' 'break' 'bright' 'broke'\n",
      " 'broken' 'brother' 'build' 'built' 'bulb' 'bulky' 'business' 'button'\n",
      " 'buy' 'buying' 'cable' 'call' 'came' 'camera' 'camping' 'cant' 'cap'\n",
      " 'car' 'card' 'care' 'carry' 'case' 'cast' 'cat' 'cause' 'certainly'\n",
      " 'chain' 'chair' 'chance' 'change' 'chapter' 'character' 'charge'\n",
      " 'charger' 'cheap' 'cheaper' 'check' 'chemistry' 'chest' 'chew' 'chewer'\n",
      " 'child' 'choice' 'christmas' 'class' 'classalinknormal' 'classic' 'clean'\n",
      " 'cleaning' 'clear' 'clip' 'close' 'coffee' 'cold' 'collar' 'collection'\n",
      " 'color' 'come' 'comfortable' 'coming' 'compact' 'company' 'compared'\n",
      " 'complaint' 'complete' 'completely' 'computer' 'con' 'condition'\n",
      " 'connection' 'construction' 'container' 'continue' 'control' 'cool'\n",
      " 'copy' 'cord' 'cost' 'could' 'couldnt' 'couple' 'course' 'cover' 'crazy'\n",
      " 'cup' 'customer' 'cut' 'cute' 'daily' 'dark' 'datahookproductlinklinked'\n",
      " 'daughter' 'day' 'deal' 'decent' 'decided' 'deep' 'definitely'\n",
      " 'described' 'description' 'design' 'designed' 'detail' 'developed'\n",
      " 'device' 'didnt' 'difference' 'different' 'difficult' 'direction'\n",
      " 'disappointed' 'disc' 'discount' 'dishwasher' 'doesnt' 'dog' 'doll'\n",
      " 'done' 'dont' 'door' 'downside' 'dream' 'dress' 'drill' 'drive' 'driver'\n",
      " 'dry' 'due' 'durable' 'dvd' 'ear' 'early' 'easier' 'easily' 'easy' 'eat'\n",
      " 'edge' 'effect' 'either' 'else' 'emotion' 'end' 'ended' 'ending' 'enjoy'\n",
      " 'enjoyable' 'enjoyed' 'enjoys' 'enough' 'entertaining' 'entire' 'episode'\n",
      " 'especially' 'etc' 'even' 'event' 'ever' 'every' 'everyday' 'everyone'\n",
      " 'everything' 'exactly' 'excellent' 'except' 'exchange' 'excited' 'expect'\n",
      " 'expected' 'expecting' 'expensive' 'experience' 'extra' 'extremely' 'eye'\n",
      " 'fabric' 'face' 'fact' 'fairly' 'fall' 'family' 'fan' 'fantastic' 'far'\n",
      " 'fast' 'father' 'favorite' 'feature' 'feel' 'feeling' 'fell' 'felt'\n",
      " 'female' 'fiction' 'fight' 'figure' 'fill' 'filled' 'film' 'filter'\n",
      " 'finally' 'find' 'finding' 'fine' 'finger' 'finish' 'finished' 'fire'\n",
      " 'first' 'fit' 'five' 'fix' 'flashlight' 'flat' 'flimsy' 'floor' 'follow'\n",
      " 'food' 'foot' 'forward' 'found' 'four' 'frame' 'free' 'friend' 'front'\n",
      " 'full' 'fun' 'function' 'funny' 'future' 'game' 'garage' 'gave' 'german'\n",
      " 'get' 'getting' 'gift' 'girl' 'give' 'given' 'giving' 'glad' 'glass'\n",
      " 'glove' 'go' 'going' 'good' 'got' 'gotten' 'granddaughter' 'grandson'\n",
      " 'great' 'green' 'grip' 'group' 'guess' 'gun' 'guy' 'hair' 'half' 'hand'\n",
      " 'handle' 'handy' 'hang' 'happen' 'happened' 'happens' 'happy' 'hard'\n",
      " 'harness' 'hat' 'hate' 'havent' 'he' 'head' 'headphone' 'hear' 'heard'\n",
      " 'heart' 'heat' 'heavier' 'heavy' 'heel' 'height' 'held' 'help' 'helpful'\n",
      " 'hero' 'heroine' 'high' 'highly' 'historical' 'history' 'hit' 'hold'\n",
      " 'holder' 'holding' 'hole' 'home' 'honest' 'hook' 'hooked' 'hope' 'hoping'\n",
      " 'hot' 'hour' 'house' 'however' 'huge' 'human' 'humor' 'husband' 'ice'\n",
      " 'id' 'idea' 'ill' 'im' 'imagine' 'immediately' 'important' 'impressed'\n",
      " 'inch' 'included' 'including' 'information' 'inside' 'install'\n",
      " 'installed' 'instead' 'instruction' 'interest' 'interested' 'interesting'\n",
      " 'ipad' 'island' 'isnt' 'issue' 'item' 'iti' 'ive' 'jean' 'job' 'john'\n",
      " 'keep' 'keeping' 'kept' 'key' 'keyboard' 'kid' 'kind' 'kindle' 'kit'\n",
      " 'kitchen' 'knee' 'knew' 'knife' 'know' 'lab' 'lamp' 'laptop' 'large'\n",
      " 'larger' 'last' 'later' 'lb' 'lead' 'learn' 'learned' 'learning' 'leash'\n",
      " 'least' 'leather' 'leave' 'led' 'left' 'leg' 'length' 'lens' 'less' 'let'\n",
      " 'level' 'lid' 'life' 'light' 'lightweight' 'likable' 'like' 'liked'\n",
      " 'line' 'list' 'litter' 'little' 'live' 'living' 'local' 'lock' 'long'\n",
      " 'longer' 'look' 'looked' 'looking' 'loose' 'lost' 'lot' 'loud' 'love'\n",
      " 'loved' 'low' 'machine' 'made' 'main' 'make' 'making' 'man' 'many' 'mat'\n",
      " 'match' 'material' 'matter' 'may' 'maybe' 'mean' 'medium' 'meet' 'men'\n",
      " 'mess' 'metal' 'middle' 'might' 'mind' 'mine' 'minute' 'missing' 'mix'\n",
      " 'mm' 'model' 'mom' 'moment' 'money' 'month' 'mostly' 'mother' 'mount'\n",
      " 'mouse' 'move' 'movie' 'moving' 'much' 'music' 'must' 'mystery' 'name'\n",
      " 'narrow' 'need' 'needed' 'negative' 'never' 'new' 'next' 'nice' 'nicely'\n",
      " 'night' 'noise' 'normal' 'normally' 'notch' 'note' 'nothing' 'noticed'\n",
      " 'novel' 'number' 'offer' 'office' 'often' 'oh' 'ok' 'okay' 'old' 'older'\n",
      " 'one' 'onei' 'open' 'opening' 'opinion' 'option' 'order' 'ordered'\n",
      " 'original' 'others' 'otherwise' 'outside' 'overall' 'owned' 'pack'\n",
      " 'package' 'pad' 'page' 'paid' 'pain' 'paint' 'painting' 'pair' 'pan'\n",
      " 'pant' 'parent' 'part' 'party' 'past' 'pay' 'people' 'perfect'\n",
      " 'perfectly' 'performance' 'period' 'person' 'pet' 'phone' 'photo' 'pick'\n",
      " 'picky' 'picture' 'piece' 'pillow' 'place' 'plan' 'plastic' 'play'\n",
      " 'played' 'player' 'playing' 'please' 'pleased' 'plenty' 'plot' 'plug'\n",
      " 'plus' 'pocket' 'point' 'poor' 'port' 'portable' 'pound' 'power'\n",
      " 'powerful' 'predictable' 'prefer' 'present' 'press' 'pretty' 'previous'\n",
      " 'price' 'pricey' 'pro' 'probably' 'problem' 'product' 'professional'\n",
      " 'project' 'properly' 'protect' 'provided' 'pull' 'pump' 'puppy'\n",
      " 'purchase' 'purchased' 'purchasing' 'purpose' 'push' 'put' 'putting'\n",
      " 'puzzle' 'quality' 'question' 'quick' 'quickly' 'quiet' 'quite' 'range'\n",
      " 'rate' 'rather' 'rating' 'read' 'reader' 'readi' 'reading' 'real'\n",
      " 'realistic' 'realize' 'really' 'reason' 'received' 'recently' 'recommend'\n",
      " 'recommended' 'red' 'regular' 'relationship' 'remember' 'remote' 'remove'\n",
      " 'replace' 'replaced' 'replacement' 'rest' 'result' 'return' 'returned'\n",
      " 'review' 'reviewer' 'ride' 'right' 'ring' 'role' 'romance' 'room'\n",
      " 'rubber' 'run' 'running' 'sad' 'safe' 'said' 'save' 'saw' 'say' 'saying'\n",
      " 'scene' 'school' 'science' 'screen' 'screw' 'season' 'seat' 'second'\n",
      " 'secure' 'see' 'seeing' 'seem' 'seemed' 'seems' 'seen' 'seller' 'sense'\n",
      " 'sensitive' 'sent' 'series' 'service' 'set' 'setting' 'several' 'sex'\n",
      " 'sexy' 'shape' 'sharp' 'sheet' 'shelf' 'shes' 'ship' 'shipping' 'shirt'\n",
      " 'shoe' 'short' 'shot' 'show' 'shower' 'side' 'sight' 'similar' 'simple'\n",
      " 'simply' 'since' 'single' 'sink' 'sister' 'sit' 'sitting' 'situation'\n",
      " 'size' 'sleep' 'slide' 'slightly' 'slip' 'slow' 'small' 'smaller' 'smart'\n",
      " 'smell' 'smooth' 'snug' 'sock' 'soft' 'solid' 'someone' 'something'\n",
      " 'sometimes' 'somewhat' 'son' 'sony' 'soon' 'sort' 'sound' 'space'\n",
      " 'speaker' 'special' 'speed' 'spend' 'stand' 'standard' 'star' 'start'\n",
      " 'started' 'starting' 'stay' 'steel' 'step' 'stick' 'stiff' 'still' 'stop'\n",
      " 'storage' 'store' 'story' 'storyline' 'straight' 'strap' 'stretchy'\n",
      " 'strong' 'stuck' 'stuff' 'sturdy' 'style' 'suction' 'summer' 'super'\n",
      " 'superb' 'support' 'supposed' 'sure' 'surface' 'surprise' 'surprised'\n",
      " 'suspense' 'sweet' 'switch' 'system' 'table' 'tablet' 'take' 'taken'\n",
      " 'taking' 'tale' 'tall' 'tank' 'tape' 'taste' 'tell' 'test' 'th' 'thank'\n",
      " 'thats' 'there' 'theyre' 'thick' 'thin' 'thing' 'think' 'thinking'\n",
      " 'third' 'though' 'thought' 'three' 'thriller' 'throw' 'tight' 'time'\n",
      " 'tiny' 'tip' 'title' 'today' 'toe' 'together' 'told' 'took' 'tool' 'top'\n",
      " 'totally' 'touch' 'tough' 'towel' 'town' 'toy' 'track' 'train' 'travel'\n",
      " 'treat' 'tree' 'tried' 'trip' 'trouble' 'true' 'truly' 'try' 'trying'\n",
      " 'turn' 'turned' 'turning' 'tv' 'twice' 'twist' 'two' 'type' 'typical'\n",
      " 'unbiased' 'understand' 'unfortunately' 'unit' 'unless' 'update' 'us'\n",
      " 'usa' 'usb' 'use' 'used' 'useful' 'using' 'usual' 'usually' 'vacuum'\n",
      " 'value' 'variety' 'version' 'video' 'view' 'volume' 'waist' 'wait'\n",
      " 'waiting' 'walk' 'walking' 'wall' 'want' 'wanted' 'wanting' 'war' 'warm'\n",
      " 'wash' 'wasnt' 'waste' 'watch' 'watched' 'watching' 'water' 'way' 'wear'\n",
      " 'wearing' 'weather' 'week' 'weight' 'well' 'went' 'weve' 'wheel' 'white'\n",
      " 'whole' 'wide' 'wife' 'window' 'wire' 'wish' 'within' 'without' 'woman'\n",
      " 'wonderful' 'wont' 'wood' 'wooden' 'word' 'work' 'worked' 'working'\n",
      " 'world' 'worn' 'worried' 'worry' 'worth' 'would' 'wouldnt' 'wow' 'write'\n",
      " 'writer' 'writing' 'written' 'wrong' 'xl' 'year' 'yellow' 'yes' 'yet'\n",
      " 'youll' 'young' 'youre' 'zipper']\n",
      "TF-IDF Vectors:\n",
      " [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.23575803 0.09644777]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.04900125 0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convert tokenized words back to full sentences\n",
    "processed_texts = [\" \".join(words) for words in tokenized_texts]\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "# Fit and transform the processed text\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "# Convert to array (if needed)\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "print(\"TF-IDF Matrix Shape:\", tfidf_matrix.shape)\n",
    "\n",
    "# Display feature names and vectors\n",
    "print(\"Feature Names:\", tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Vectors:\\n\", tfidf_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Vector: (100,)\n",
      "AI-Generated Vector: (100,)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "import numpy as np\n",
    "\n",
    "# Sample corpus\n",
    "sentences = [[\"this\", \"is\", \"a\", \"test\"], [\"word\", \"embeddings\", \"are\", \"powerful\"]]\n",
    "\n",
    "# Train FastText model\n",
    "fasttext_model = FastText(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "def sentence_to_vector(sentence, model, vector_size=100):\n",
    "    words = sentence.split()\n",
    "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    return np.mean(word_vectors, axis=0) if word_vectors else np.zeros(vector_size)\n",
    "\n",
    "# Example sentences\n",
    "original_text = \"word embeddings are powerful\"\n",
    "ai_generated_text = \"this is a generated text\"\n",
    "\n",
    "original_vector = sentence_to_vector(original_text, fasttext_model)\n",
    "ai_vector = sentence_to_vector(ai_generated_text, fasttext_model)\n",
    "\n",
    "print(\"Original Vector:\", original_vector.shape)\n",
    "print(\"AI-Generated Vector:\", ai_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mradu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\mradu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mradu\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT Original Vector: (384,)\n",
      "SBERT AI-Generated Vector: (384,)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load SBERT model\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Get sentence embeddings\n",
    "original_vector = sbert_model.encode(original_text)\n",
    "ai_vector = sbert_model.encode(ai_generated_text)\n",
    "\n",
    "print(\"SBERT Original Vector:\", original_vector.shape)  # Output: (384,)\n",
    "print(\"SBERT AI-Generated Vector:\", ai_vector.shape)  # Output: (384,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.2150\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Reshape the vectors for sklearn's cosine_similarity function\n",
    "original_vector = original_vector.reshape(1, -1)\n",
    "ai_vector = ai_vector.reshape(1, -1)\n",
    "\n",
    "# Compute cosine similarity\n",
    "similarity_score = cosine_similarity(original_vector, ai_vector)[0][0]\n",
    "\n",
    "print(f\"Cosine Similarity: {similarity_score:.4f}\")  # Closer to 1 means more similar\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
